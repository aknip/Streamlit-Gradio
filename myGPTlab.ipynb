{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aknip/Streamlit-Gradio/blob/main/myGPTlab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# myGPTlab\n",
        "\n",
        "This app helps to process texts (longform content) with ChatGPT via API.\n",
        "\n",
        "## Start:\n",
        "**Run all cells - that's it.**\n",
        "- The notebooks checks automatically, if an initial setup (with PIP etc.) is necessary. The setup status is saved in the file 'installation.done'\n",
        "- You can force the setup by deleting this file or by going to Seciton \"Setup and Configuration\" and checking \"inital_setup_mode\". Afterwards uncheck \"Setup and Configuration\".\n",
        "\n",
        "## Working with myGPTlab\n",
        "Lorem ipsum...\n"
      ],
      "metadata": {
        "id": "Ht3Wkxk7Rpn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Configuration"
      ],
      "metadata": {
        "id": "2goGpKmVRRUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Settings\n",
        "\n",
        "# @markdown Default model\n",
        "default_model = 'GPT-3.5' # @param [\"GPT-3.5\", \"GPT-4\"]\n",
        "\n",
        "# @markdown Start Gradio webapp.\n",
        "start_gradio_webapp = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Initial Setup Mode for pip install, fetch credentials etc.\n",
        "initial_setup_mode = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Debug Mode for extensive logging.\n",
        "debug_mode = True # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown iOS Mode to develop helper functions, no Gradio.\n",
        "# @markdown Useful for development on iOS, eg. with Carnets App\n",
        "ios_mode = False # @param {type:\"boolean\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9NyekTkaGU3Z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folders = {\n",
        "    'audio': 'audio',\n",
        "    'audio-chunks': 'audio/chunks',\n",
        "    'transcript':'audio-transcript',\n",
        "    'transcript-chunks': 'audio-transcript/chunks',\n",
        "    'text-input': 'text-input',\n",
        "    'text-input-backup': 'text-input-backup'\n",
        "}"
      ],
      "metadata": {
        "id": "FBJns82oduIg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import widgets\n",
        "from IPython.display import Javascript, display, clear_output\n",
        "notify_output = widgets.Output()\n",
        "display(notify_output)\n",
        "@notify_output.capture()\n",
        "def popup(text):\n",
        "    clear_output()\n",
        "    display(Javascript(\"alert('{}')\".format(text)))\n",
        "#popup('Hello World!')"
      ],
      "metadata": {
        "id": "Z-9rgUetGeu0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "902225593f3b40eea134dc76f3f9d808",
            "d1ce91b752544c9aa636f98938dd879b"
          ]
        },
        "outputId": "44f07fa0-2820-4053-c76f-1fbd1c0eb1ca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "902225593f3b40eea134dc76f3f9d808"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if initial_setup_mode != True:\n",
        "  if os.path.exists('installation.done'):\n",
        "      initial_setup_mode = False\n",
        "      print('No initial setup - forced by existing file \"installation.done\"')\n",
        "  else:\n",
        "    initial_setup_mode = True\n",
        "    print('Starting automatic setup - forced by missing file \"installation.done\".\\n\\nEnter API Keys as JSON (in next notebook cell).')\n",
        "    popup('Starting automatic setup. Enter API Keys as JSON (in next notebook cell).')\n",
        "else:\n",
        "  print('Starting setup.\\n\\nEnter API Keys as JSON (in next notebook cell).')\n",
        "  popup('Starting setup. Enter API Keys as JSON (in next notebook cell).')"
      ],
      "metadata": {
        "id": "4cDDPr809eC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebdcb800-0cd7-498b-a37e-7d482695deba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No initial setup - forced by existing file \"installation.done\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if initial_setup_mode == True:\n",
        "  #popup('Enter API Keys as JSON:')\n",
        "  !wget -q bit.ly/aknip-colab-setup\n",
        "  %run aknip-colab-setup\n",
        "else:\n",
        "  print('No initial setup.')"
      ],
      "metadata": {
        "id": "eepWvnFN-yvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311ef1ea-bf3f-4c9e-8026-93f32c65bb51"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No initial setup.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "creds = json.loads(os.getenv('CREDS'))\n",
        "# openAI_key = creds['OpenAI']['v2']['credential']\n",
        "# print(openAI_key)"
      ],
      "metadata": {
        "id": "iEwJ4-ShAt0v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if ios_mode == False:\n",
        "  print('Mac')\n",
        "else:\n",
        "  print('iOS')"
      ],
      "metadata": {
        "id": "DRWVZHV1MOij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25bf239d-17e5-454c-d952-474c8dd25fab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Wtw0CARBQhTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7576378e-8009-4511-b6e8-1b053ffd3833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No initial setup.\n"
          ]
        }
      ],
      "source": [
        "if initial_setup_mode == True:\n",
        "  !pip install openai==0.27.7 yt-dlp==2023.7.6 librosa==0.10.0.post2 pickle-mixin==1.0.2 langchain==0.0.225 PyPDF2==3.0.1 PyMuPDF==1.22.5 -q\n",
        "else:\n",
        "  print('No initial setup.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if (initial_setup_mode == True) and (ios_mode == False) :\n",
        "  !pip install gradio -q\n",
        "else:\n",
        "  print('No initial setup / iOS.')"
      ],
      "metadata": {
        "id": "-TpG9Zr0IUU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b92c38-08ff-4fd4-de2b-474263970f12"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No initial setup / iOS.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if (initial_setup_mode == True) and (ios_mode == False) :\n",
        "  %load_ext gradio\n",
        "else:\n",
        "  print('No initial setup / iOS.')"
      ],
      "metadata": {
        "id": "EyhkzcJ_Ob45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57febb56-2fca-4d32-ab3b-1a06eb7780e9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No initial setup / iOS.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if initial_setup_mode == True:\n",
        "  f= open('installation.done','w+')\n",
        "  f.close()\n",
        "  print('Initial setup done. Application starting.')\n",
        "  popup('Initial setup done. Application starting.')\n",
        "else:\n",
        "  print('No initial setup.')"
      ],
      "metadata": {
        "id": "l0Lx1eHL9Nik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ae8580-38f4-41f5-c624-657e5511db4b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No initial setup.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions\n",
        "\n",
        "- **create_file_directory**: Creates a new directory - if it not exists yet. The always_delete flag forces a deletion even if it exists."
      ],
      "metadata": {
        "id": "zO7UxoRjSWUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# v2 - 06.08.2023\n",
        "import shutil\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "def create_file_directory(directory, always_delete=False):\n",
        "  # Creates a new directory - if it not exists yet. The always_delete flag forces a deletion even if it exists.\n",
        "  # Examples:\n",
        "  # - create_file_directory('texts', False) => creates a new directory only if it not exists yet\n",
        "  # - create_file_directory('texts', True) => always deletes existing directory and creates a new one\n",
        "  if os.path.exists(directory):\n",
        "    if always_delete:\n",
        "      # delete the diectory recursively\n",
        "      shutil.rmtree(directory)\n",
        "  # create directory\n",
        "  if not os.path.exists(directory):\n",
        "    os.mkdir(directory)\n",
        "\n",
        "\n",
        "def find_files(path, extensions=[\".txt\"], recursive=False):\n",
        "    # Recursively (optional) find all files with extension in path\n",
        "    my_files = []\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for f in files:\n",
        "            if extensions == []:\n",
        "                my_files.append(os.path.join(root, f))\n",
        "            else:\n",
        "                for ext in extensions:\n",
        "                    if f.endswith(ext):\n",
        "                        my_files.append(os.path.join(root, f))\n",
        "        # no recursion / don't look inside any subdirectory\n",
        "        if recursive == False:\n",
        "            break\n",
        "    return my_files\n",
        "\n",
        "\n",
        "def merge_textfiles(path, extensions=[\".txt\"], recursive=False, new_filename='merged.txt'):\n",
        "    # Recursively (optional) find all files with extension in path\n",
        "    my_files = find_files(path, extensions, recursive)\n",
        "    merged_text = ''\n",
        "    for filename in my_files:\n",
        "      # print(filename)\n",
        "      f= open(filename,'r')\n",
        "      if f.mode == 'r':\n",
        "            contents =f.read()\n",
        "      f.close()\n",
        "      merged_text = merged_text + contents + '\\n\\n\\n'\n",
        "\n",
        "    f= open(new_filename,'w+')\n",
        "    f.write(merged_text)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "dydmkY6_SZeG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The App"
      ],
      "metadata": {
        "id": "A5nBZppW-bt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if (ios_mode == False) and (start_gradio_webapp == True):\n",
        "  import gradio as gr\n",
        "\n",
        "  # Theming\n",
        "  theme = gr.themes.Default(\n",
        "      primary_hue=\"slate\" # , radius_size=gr.themes.Size(radius_sm=\"3px\", radius_xs=\"2px\", radius_xxs=\"1px\")\n",
        "  )\n",
        "  # Styling: Change max width\n",
        "  css = \"\"\"\n",
        "    .gradio-container {max-width: 700px!important}\n",
        "    .vspacer1 {margin-top: 50px}\n",
        "  \"\"\"\n",
        "\n",
        "  with gr.Blocks(theme=theme, css=css) as demo:\n",
        "\n",
        "      gr.Markdown(\"# ChatGPTLab 2.0\", elem_classes=\"vspacer1\")\n",
        "      gr.Markdown(\"### Optimizing your work with LLMs.\")\n",
        "\n",
        "      project_name = gr.Textbox(label=\"Project name\")\n",
        "\n",
        "      #\n",
        "      # 1. Input Text\n",
        "      #\n",
        "      with gr.Tab(\"Input Text \"):\n",
        "        gr.Markdown(\"Please enter text\")\n",
        "\n",
        "        # Input text via UI\n",
        "        gr.Markdown(\"### Input your text:\")\n",
        "        text_input = gr.Textbox(label=\"Enter text\", placeholder=\"Your text here...\", lines=10)\n",
        "        text_output = gr.Textbox(label=\"Result\")\n",
        "\n",
        "        def text_save(text, proj_name):\n",
        "          create_file_directory(proj_name, False)\n",
        "          create_file_directory(proj_name + '/' +  folders['text-input'], False)\n",
        "          f= open(proj_name + '/' +  folders['text-input'] + '/input_text.txt','w+')\n",
        "          f.write(text)\n",
        "          f.close()\n",
        "          log_text = \"Text saved.\"\n",
        "          return log_text\n",
        "        text_button = gr.Button(\"Save text\")\n",
        "        text_button.click(text_save, [text_input, project_name], text_output)\n",
        "\n",
        "        gr.Markdown(\"\")\n",
        "        gr.Markdown(\"\")\n",
        "\n",
        "        # Input text via upload\n",
        "        gr.Markdown(\"### Or upload your text:\")\n",
        "        upload_button = gr.UploadButton(\"Click to Upload a File\", file_types=[\".txt\",\".md\"], file_count=\"single\")\n",
        "        file_output = gr.Textbox(label=\"Result\")\n",
        "\n",
        "        def upload_file(my_file, proj_name):\n",
        "          create_file_directory(proj_name, False)\n",
        "          create_file_directory(proj_name + '/' +  folders['text-input'], False)\n",
        "          # copy to project directory\n",
        "          full_upload_path = my_file.name\n",
        "          just_the_filename = os.path.basename(full_upload_path)\n",
        "          full_text_path = \"./\" + proj_name + '/' + folders['text-input'] + '/' + just_the_filename\n",
        "          shutil.copyfile(full_upload_path, full_text_path)\n",
        "          # check if file is empty\n",
        "          f= open(full_text_path,'r')\n",
        "          if f.mode == 'r': contents =f.read()\n",
        "          f.close()\n",
        "          log_text = just_the_filename + \"\\n\"\n",
        "          if len(contents) == 0:\n",
        "            log_text = log_text + \"Error: Upload file lengt 0 bytes\"\n",
        "          else:\n",
        "            log_text = log_text + \"Upload successful\"\n",
        "          return log_text\n",
        "        upload_button.upload(upload_file, [upload_button, project_name], file_output)\n",
        "\n",
        "      #\n",
        "      # 2. Download full project\n",
        "      #\n",
        "      with gr.Tab(\"Download\"):\n",
        "        gr.Markdown(\"Download full project as ZIP file.\")\n",
        "        download_button = gr.Button(\"Download project\")\n",
        "        download_output = gr.File()\n",
        "\n",
        "        def download_do(proj_name):\n",
        "          full_text_path = \"./\" + proj_name\n",
        "          shutil.make_archive('archive', 'zip', full_text_path)\n",
        "          result = \"Downloading \" + full_text_path\n",
        "          return \"archive.zip\"\n",
        "        download_button.click(download_do, project_name, download_output)\n",
        "\n",
        "      #\n",
        "      # 3. xxx\n",
        "      #\n",
        "      with gr.Tab(\"Step 2\"):\n",
        "        gr.Markdown(\"Please select the optimization:\")\n",
        "        radio = gr.Radio(\n",
        "          [\"by headline\", \"by paragraph\", \"by §§§\"], label=\"Text split method\"\n",
        "        )\n",
        "        name = gr.Textbox(label=\"Name\", placeholder=\"Enter text...\")\n",
        "        output = gr.Textbox(label=\"Output Box\")\n",
        "        greet_btn = gr.Button(\"Start\", scale=0)\n",
        "        def greet(name):\n",
        "          result = \"HALLO \" + name + \"!!!\"\n",
        "          return result\n",
        "        greet_btn.click(fn=greet, inputs=name, outputs=output, api_name=\"greet\")\n",
        "\n",
        "  demo.launch(quiet=True, share=True, debug=debug_mode)\n",
        "\n",
        "else:\n",
        "  print('iOS Mode - Nothing to do.')"
      ],
      "metadata": {
        "id": "xK8-6hL0eIIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc4bc64-c41a-4d23-ac32-41377e4659a7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iOS Mode - Nothing to do.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT"
      ],
      "metadata": {
        "id": "lKcS3L5pXA06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Promptlib\n",
        "# Todo: promptlib not as global,\n",
        "\n",
        "import pickle\n",
        "\n",
        "# myprompt = \"The given text is delimited by triple backticks. Summarize the current text to succint and clear bullet points of its contents.\n",
        "#          The length of the summary must be 200 words maximum.\"\n",
        "# myprompt = \"The given text is delimited by triple backticks.\n",
        "#           Summarize the current text to a maximum of 15 succint and clear bullet points of its contents.\"\n",
        "# myprompt = myprompt + \"The maximum number of words should be 300 words in total. \"\n",
        "# myprompt = myprompt + \"Write everything in German language. \" # this is optional !\n",
        "# myprompt = myprompt + \"```\" + input_text + \"```\"\n",
        "\n",
        "\n",
        "\n",
        "promptlib = {\n",
        "        'sum': {\n",
        "            'description': 'Max word length: 1100 for EN, 900 for DE. approx. 1 min processing time Example: Full book: 113 parts á 1000 words takes 15 min.',\n",
        "            'category': 'summarize',\n",
        "            '1': {\n",
        "                'note': 'summarize prompt v1',\n",
        "                'prompt': 'The given text is delimited by triple backticks. Summarize the current text to succint and clear bullet points of its contents. {language_text} {length_text} Text:```{input_text}```',\n",
        "                'lang-de': 'Write everything in German language.',\n",
        "                'lang-en': 'write everything in EN.',\n",
        "                'lang-same': 'write in language of original text.',\n",
        "                'length-max-fix': 'The length of the summary must be 200 words maximum.',\n",
        "                'length-max-dyn': 'max length is {{length_calc(\"{input_text}\", {max_len})}} words.'\n",
        "            },\n",
        "            '2': {\n",
        "                'note': 'summarize prompt v2',\n",
        "                'prompt': 'The given text is delimited by triple backticks. Summarize the current text to a maximum of 15 succint and clear bullet points of its contents. {language_text} {length_text} Text:```{input_text}```',\n",
        "                'lang-de': 'Write everything in German language.',\n",
        "                'lang-en': 'write everything in EN.',\n",
        "                'lang-same': 'write in language of original text.',\n",
        "                'length-max-fix': 'The maximum number of words should be 300 words in total.',\n",
        "                'length-max-dyn': 'max length is {{length_calc(\"{input_text}\", {max_len})}} words.'\n",
        "            }\n",
        "        },\n",
        "        'para': {\n",
        "            'description': 'paraphrase prompt',\n",
        "            'category': 'paraphrase',\n",
        "            '1': {\n",
        "                'note': 'summarize prompt',\n",
        "                'prompt': 'hello, this is prompttext para v1. {language_text} {length_text} Text:```{input_text}```',\n",
        "                'lang-de': 'write evertyhing in DE.',\n",
        "                'lang-en': 'write everything in EN.',\n",
        "                'lang-same': 'write in language of original text.',\n",
        "                'length-max-fix': 'max length is 100 words.',\n",
        "                'length-max-dyn': 'max length is {{length_calc(\"{input_text}\", {max_len})}} words.'\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "def write_prompt_to_lib(prompt):\n",
        "    # adds or updates prompt to library\n",
        "    id = prompt['id']\n",
        "    version = prompt['version']\n",
        "    if id not in promptlib:\n",
        "        # new id\n",
        "        promptlib[id] = {}\n",
        "        promptlib[id][version] = {}\n",
        "    else:\n",
        "        # id already existing, checking for version\n",
        "        if version not in promptlib[id]:\n",
        "            promptlib[id][version] = {}\n",
        "    promptlib[id]['description'] = prompt['description']\n",
        "    promptlib[id]['category'] = prompt['category']\n",
        "    promptlib[id][version]['note'] = prompt['note']\n",
        "    promptlib[id][version]['prompt'] = prompt['prompt']\n",
        "\n",
        "\n",
        "def get_prompt_from_lib(id=None, version=None):\n",
        "    # searches for prompt with given id\n",
        "    # looks for highest available version, if no version is given\n",
        "    if id==None:\n",
        "        prompt_found = None\n",
        "    else:\n",
        "        prompt = promptlib[id]\n",
        "        if version != None:\n",
        "            # version given\n",
        "            version_highest = int(version)\n",
        "        else:\n",
        "            # look for highest available version\n",
        "            version_highest = 0\n",
        "            for key, val in prompt.items():\n",
        "                try:\n",
        "                    version = int(key)\n",
        "                except ValueError:\n",
        "                    version = 0\n",
        "                if version > version_highest:\n",
        "                    version_highest = version\n",
        "        prompt_version = prompt[str(version_highest)]\n",
        "        # return dict\n",
        "        prompt_found = {}\n",
        "        prompt_found['id'] = id\n",
        "        prompt_found['description'] = prompt['description']\n",
        "        prompt_found['category'] = prompt['category']\n",
        "        prompt_found['version'] = str(version_highest)\n",
        "        prompt_found['note'] = prompt_version['note']\n",
        "        prompt_found['prompt'] = prompt_version['prompt']\n",
        "        prompt_found['lang-de'] = prompt_version['lang-de']\n",
        "        prompt_found['lang-en'] = prompt_version['lang-en']\n",
        "        prompt_found['lang-same'] = prompt_version['lang-same']\n",
        "        prompt_found['length-max-fix'] = prompt_version['length-max-fix']\n",
        "        prompt_found['length-max-dyn'] = prompt_version['length-max-dyn']\n",
        "    return prompt_found\n",
        "\n",
        "def build_prompt_from_template(input_txt, prompt_obj, lang, length='', length_max=1):\n",
        "  prompt_text =  prompt_obj['prompt']\n",
        "  language_text = prompt_obj[lang]\n",
        "  # calculate max. output lengt dynamically\n",
        "  if length != '':\n",
        "    length_template = prompt_obj[length]\n",
        "    length_template2 = length_template.format(input_text = input_txt, max_len=length_max)\n",
        "    length_text = eval(f\"f'{length_template2}'\")\n",
        "  else:\n",
        "    length_text = ''\n",
        "  final_prompt = prompt_text.format(language_text=language_text, length_text=length_text, input_text=input_txt)\n",
        "  return final_prompt\n",
        "\n",
        "def length_calc(my_text, maxWords):\n",
        "  # maxWords 0.0 - 1.0 (percentage)\n",
        "    return int(len(my_text) * maxWords)\n",
        "\n",
        "def save_promptlib(promptlib):\n",
        "  # saves promptlib to disk\n",
        "  with open('promptlib.dictionary', 'wb') as dict_file:\n",
        "      pickle.dump(promptlib, dict_file)\n",
        "\n",
        "def load_promptlib():\n",
        "  # load promptlib from disk\n",
        "  with open('promptlib.dictionary', 'rb') as dict_file:\n",
        "      promptlib = pickle.load(dict_file)\n",
        "  return promptlib\n",
        "\n",
        "def text_stats(input_text):\n",
        "    nr_paragraphs = len(input_text.split(\"\\n\"))\n",
        "    nr_words = wordcount(input_text)\n",
        "    array_segments = input_text.split(\"§§§\")\n",
        "    nr_segments = len(array_segments)\n",
        "    stat_txt = f'Number of words / paragraphs / §§§ segements: {nr_words} / {nr_paragraphs} / {nr_segments}'\n",
        "    if nr_segments > 1:\n",
        "        seg_stats = []\n",
        "        seg_stats_text = ''\n",
        "        for seg in array_segments:\n",
        "            seg_stats.append(wordcount(seg))\n",
        "            seg_stats_text = seg_stats_text + str(wordcount(seg)) + ' / '\n",
        "        stat_txt = stat_txt + '\\n\\nMax words in §§§ segment: ' + str(max(seg_stats)) + ' (' + seg_stats_text[:-3] + ')'\n",
        "    return stat_txt\n",
        "\n",
        "def wordcount(input_text):\n",
        "    # returns number of words of given input_text\n",
        "    return len(input_text.split())\n",
        "\n",
        "import random\n",
        "def split_text_by_separator (text, separator, joiner, fixer, min_words=1000, max_words=0):\n",
        "    # Split text into an array of texts with a maximum word count\n",
        "    # optionally use random word counts between min_words and max_words (if max_words is set)\n",
        "    # example 1: Split by paragraphs\n",
        "    #   split_text_by_separator(input_text, '\\n', '\\n', 'fix-nothing', 2000)\n",
        "    # example 2: Split by sentences '. ' and fix\n",
        "    #   split_text_by_separator(input_text, '. ', ' ', 'fix-end', 2000)\n",
        "    # example 3: Split by markdown headlines '\\n#' and fix\n",
        "    #   split_text_by_separator(input_text, '\\n#', '\\n\\n', 'fix-start', 2000)\n",
        "    paragraphs = []\n",
        "    sections = []\n",
        "    section = ''\n",
        "    if max_words == 0:\n",
        "      max_words = min_words\n",
        "    # Split text, separated by separator\n",
        "    paragraphs_stripped = text.split(separator)\n",
        "    # fix paragraphs (if wanted) by re-adding the separator at end or start\n",
        "    if fixer == 'fix-nothing':\n",
        "        paragraphs = paragraphs_stripped\n",
        "    if fixer == 'fix-end':\n",
        "        for index, paragraph in enumerate(paragraphs_stripped):\n",
        "            if (index+1) == len(paragraphs_stripped):\n",
        "                paragraphs.append(paragraph)\n",
        "            else:\n",
        "                paragraphs.append(paragraph + separator)\n",
        "    if fixer == 'fix-start':\n",
        "        for index, paragraph in enumerate(paragraphs_stripped):\n",
        "            if index == 0:\n",
        "                paragraphs.append(paragraph)\n",
        "            else:\n",
        "                paragraphs.append(separator + paragraph)\n",
        "    # Loop through paragraphs and aggregate up to maximum word count\n",
        "    for index, paragraph in enumerate(paragraphs):\n",
        "        if min_words == max_words:\n",
        "            max_random = min_words\n",
        "        else:\n",
        "            max_random = random.randrange(min_words, max_words)\n",
        "        test_section = section + paragraph + joiner\n",
        "        if wordcount(test_section) > max_random:\n",
        "            sections.append(section.strip())\n",
        "            section = paragraph\n",
        "            if min_words == max_words:\n",
        "                max_random = min_words\n",
        "            else:\n",
        "                max_random = random.randrange(min_words, max_words)\n",
        "        else:\n",
        "            section = section + paragraph + joiner\n",
        "            # if last paragraph, append to array\n",
        "            if (index+1) == len(paragraphs):\n",
        "              sections.append(section)\n",
        "    return sections"
      ],
      "metadata": {
        "id": "VD9mE55Zkl--"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_text = 'Die Photosynthese ist ein physiologischer Prozess zur Erzeugung energiereicher Biomoleküle aus energieärmeren Stoffen mit Hilfe\\\n",
        " von Lichtenergie. Sie wird von Pflanzen, Algen und manchen Bakterien betrieben. Bei diesem biochemischen Vorgang wird Lichtenergie mit Hilfe\\\n",
        " von lichtabsorbierenden Farbstoffen wie Chlorophyll in chemische Energie umgewandelt. Diese wird dann genutzt, um aus energiearmen\\\n",
        " anorganischen Stoffen (vor allem Kohlenstoffdioxid (CO2) und Wasser (H2O)) energiereiche organische Verbindungen (vor allem Kohlenhydrate)\\\n",
        " aufzubauen. Der genutzte Anteil der eingestrahlten Energie, nämlich der zum Aufbau der Assimilate verwendete Anteil, wird photosynthetische\\\n",
        " Effizienz genannt. Soweit die energiereichen organischen Stoffe zu Bestandteilen des Lebewesens werden, bezeichnet man deren Synthese als\\\n",
        " Assimilation. Man unterscheidet zwischen oxygener und anoxygener Photosynthese. Bei der oxygenen Photosynthese wird molekularer\\\n",
        " Sauerstoff (O2) freigesetzt. Bei der anoxygenen Photosynthese, die nur von Bakterien betrieben wird, entstehen statt Sauerstoff andere\\\n",
        " anorganische Stoffe, beispielsweise elementarer Schwefel (S). Die Photosynthese ist der einzige biochemische Prozess, bei dem\\\n",
        " Lichtenergie, meistens Sonnenlicht, in chemisch gebundene Energie umgewandelt wird (Phototrophie). Indirekt sind auch fast alle\\\n",
        " heterotrophen (nicht zur Photosynthese fähigen) Lebewesen von ihr abhängig, da sie der Photosynthese letztlich ihre Nahrung und auch\\\n",
        " den zur Energiegewinnung mittels aerober Atmung benötigten Sauerstoff verdanken. Aus dem Sauerstoff entsteht außerdem die schützende\\\n",
        " Ozonschicht der Erdatmosphäre.'\n",
        "#print(my_text)\n",
        "print(textwrap.fill(my_text, 120) + '\\n')"
      ],
      "metadata": {
        "id": "ok-VAVRsVTR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8578feb-09b8-4f9b-b939-17e42f3510e8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Photosynthese ist ein physiologischer Prozess zur Erzeugung energiereicher Biomoleküle aus energieärmeren Stoffen\n",
            "mit Hilfe von Lichtenergie. Sie wird von Pflanzen, Algen und manchen Bakterien betrieben. Bei diesem biochemischen\n",
            "Vorgang wird Lichtenergie mit Hilfe von lichtabsorbierenden Farbstoffen wie Chlorophyll in chemische Energie\n",
            "umgewandelt. Diese wird dann genutzt, um aus energiearmen anorganischen Stoffen (vor allem Kohlenstoffdioxid (CO2) und\n",
            "Wasser (H2O)) energiereiche organische Verbindungen (vor allem Kohlenhydrate) aufzubauen. Der genutzte Anteil der\n",
            "eingestrahlten Energie, nämlich der zum Aufbau der Assimilate verwendete Anteil, wird photosynthetische Effizienz\n",
            "genannt. Soweit die energiereichen organischen Stoffe zu Bestandteilen des Lebewesens werden, bezeichnet man deren\n",
            "Synthese als Assimilation. Man unterscheidet zwischen oxygener und anoxygener Photosynthese. Bei der oxygenen\n",
            "Photosynthese wird molekularer Sauerstoff (O2) freigesetzt. Bei der anoxygenen Photosynthese, die nur von Bakterien\n",
            "betrieben wird, entstehen statt Sauerstoff andere anorganische Stoffe, beispielsweise elementarer Schwefel (S). Die\n",
            "Photosynthese ist der einzige biochemische Prozess, bei dem Lichtenergie, meistens Sonnenlicht, in chemisch gebundene\n",
            "Energie umgewandelt wird (Phototrophie). Indirekt sind auch fast alle heterotrophen (nicht zur Photosynthese fähigen)\n",
            "Lebewesen von ihr abhängig, da sie der Photosynthese letztlich ihre Nahrung und auch den zur Energiegewinnung mittels\n",
            "aerober Atmung benötigten Sauerstoff verdanken. Aus dem Sauerstoff entsteht außerdem die schützende Ozonschicht der\n",
            "Erdatmosphäre.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_text2 = f'''\n",
        "# The Potential of AI in Education\n",
        "\n",
        "So, anyone who's been paying attention for the last few months has been seeing headlines like this, especially in education. The thesis has been students are going to be using chat GPT and other forms of AI to cheat, do their assignments, they're not going to learn, and it's going to completely undermine education as we know it.\n",
        "\n",
        "Now what I'm going to argue today is not only are there ways to mitigate all of that, if we put the right guardrails, we do the right things, we can mitigate it, but I think we're at the cusp of using AI for probably the biggest positive transformation that education has ever seen. And the way we're going to do that is by giving every student on the planet an artificially intelligent but amazing personal tutor, and we're going to give every teacher on the planet an amazing, artificially intelligent teaching assistant.  And just to appreciate how big of a deal it would be to give everyone a personal tutor, I show you this clip from Benjamin Bloom's 1984 two-sigma study, or he called it the two-sigma problem.\n",
        "\n",
        "The two-sigma comes from two standard deviations, sigma the symbol for standard deviation, and he had good data that showed that, look, a normal distribution, that's the one that you see in the traditional bell curve right in the middle, that's how the world kind of sorts itself out, that if you were to give personal one-to-one tutoring for students, that you could actually get a distribution that looks like that right, it says tutorial one-to-one with the asterisks, like that right distribution, a two standard deviation improvement. Just to put that in plain language, that could take your average student and turn them into an exceptional student, it can take your below average student and turn them into an above average student.\n",
        "\n",
        "# The Challenge of Scaling Personalized Instruction\n",
        "\n",
        "Now, the reason why he framed it as a problem was he said, well, this is all good, but how do you actually scale group instruction this way? How do you actually give it to everyone in an economic way? What I'm about to show you is, I think, the first moves towards doing that. Obviously, we've been trying to approximate it in some way at Khan Academy for over a decade now, but I think we're at the cusp of accelerating it dramatically.  I'm going to show you the early stages of what RAI, which we call Khan Migo, what it can now do, and maybe a little bit of where it is actually going.  This right over here is a traditional exercise that you or many of your children might have seen on Khan Academy, but what's new is that little bot thing at the right, and we'll start by seeing one of the very important safeguards, which is the conversation is recorded and viewable by your teacher.  It's moderated, actually, by a second AI, and also, it does not tell you the answer.  It is not a cheating tool.  Notice, when the student says, tell me the answer, it says, I'm your tutor.\n",
        "\n",
        "What do you think is the next step for solving the problem? Now, if the student makes a mistake, and this will surprise people who think large language models are not good at mathematics, notice not only does it notice the mistake, it asks the student to explain their reasoning, but it's actually doing what I would say not just even an average tutor would do, but an excellent tutor would do. It's actually able to divine what is probably the misconception in that student's mind, that they probably didn't use the distributive properly.  Remember, we need to distribute the negative two to both the nine and the 2M inside of the parentheses.  This to me is a very, very, very big deal, and it's not just in math.  This is a computer programming exercise on Khan Academy where the student needs to make the clouds part, and so we can see the student starts defining a variable, left X minus minus.\n",
        "\n",
        "# AI as a Super Tutor\n",
        "\n",
        "It only made the left cloud part, but then they can ask a con amigo, what's going on? Why is only the left cloud moving? And it understands the code. It knows all the context of what the student is doing, and it understands that those ellipses are there to draw clouds, which I think is kind of mind-blowing, and it says, to make the right cloud move as well, try adding a line of code inside the draw function that increments the right X variable by one pixel in each frame.  Now, this one is maybe even more amazing, because we have a lot of math teachers.  We've all been trying to teach the world to code, but there aren't a lot of computing teachers out there, and what you just saw, even when I'm tutoring my kids when they're learning to code, I can't help them this well, this fast.  This is really going to be a super tutor.  And it's not just exercises.  It understands what you're watching.\n",
        "\n",
        "It understands the context of your video. It can answer the age-old question, why do I need to learn this? And it asks, socratically, well, what do you care about? And let's say the student says, I want to be a professional athlete, and it says, well, learning about the size of cells, which is what this video is about, that could be really useful for understanding nutrition and how your body works, et cetera.  It can answer questions.  It can quiz you.  It can connect it to other ideas.  You can now ask as many questions of a video as you could ever dream of.  So another big shortage out there, I remember the high school I went to, the student-to-guidance counselor ratio was about 200 or 300 to 1.  A lot of the country, it's worse than that.  We can use Conmigo to give every student a guidance counselor, academic coach, career coach, life coach, which is exactly what you see right over here.  And this is all, you know, we launched this with the GPT-4 launch.  We have a few thousand people on this.  This isn't a fake demo.\n",
        "\n",
        "§§§\n",
        "\n",
        "# Conversations with Historical Figures and AI Debates\n",
        "\n",
        "This is really in action. And then there's, you know, things that I think it would have been even harder, it would have been a little science fiction to do with even a traditional tutor.  There's a student, we run an online high school with Arizona State University called Khan World School.  And we have a student who attends that online school based in India, her name is Sanvi, and she was doing a report on the Great Gatsby.  And when she was reading the Great Gatsby, Jay Gatsby keeps looking at the green light off into the distance.  And she's like, well, why does he do that? She did some web searches, and people have obviously studied this and commented about the symbolism of that, but none of it was really resonating with her.  And then she realized that she had Conmigo and that she could talk to Jay Gatsby himself.\n",
        "\n",
        "# Unlocking Learning Literature and AI Debates\n",
        "\n",
        "And so, ah, splendid choice, old sport, I am now Jay Gatsby, the enigmatic millionaire from Escott Jarrell's classic. And so why do you keep staring at the green light? Ah, the green light, old sport.  It's a symbol of my dreams and desires, you see.  It's situated at the end of Daisy Buchanan's dock, across the bay from my mansion.  I gaze at it longingly as it represents my yearning for the past and my hope to reunite with Daisy, the love of my life.  And what was cool is Sanvi had said, I had this long conversation, she called him Mr.  Gatsby.  And at the end, she actually apologized for taking his time, which I thought was very polite of her.  But you can imagine, this unlocks learning literature, learning, you can talk to historical figures, we're even probably going to add an activity, you can talk to, like, the Mississippi River.  It brings things to life in ways that really were science fiction even six months or a year ago.  Students can get into debates with the AI.  And we've got, here's the students debating whether we should cancel student debt.  The student is against canceling student debt.  And we've gotten very clear feedback.  We've started running it at Con World School and our lab school that we have, Con Lab School.  The students, the high school students especially, they're saying, this is amazing to be able to fine tune my arguments without fearing judgment.\n",
        "\n",
        "# Enhancing Language Arts with AI Writing Collaboration\n",
        "\n",
        "It makes me that much more confident to kind of go into the classroom and really participate. And we all know that Socratic dialogue debate is a great way to learn.  But frankly, it's not out there for most students.  But now it can be accessible to hopefully everyone.  A lot of the narrative, we saw that in the headlines, has been it's going to do the writing for kids.  Kids are not going to learn to write.  But we are showing that there's ways that the AI doesn't write for you.  It writes with you.  So this is a little thing.  And my eight-year-old is addicted to this.  And he's not a kid that really liked writing before.  But where, you know, you could say, I want to write a horror story.  And it says, ooh, a horror story.  How spine-tingling and thrilling.  Let's dive into the world of eerie shadows and chilling mysteries.\n",
        "\n",
        "And this is an activity where the student will write two sentences. And then the AI will write two sentences.  And so they collaborate together on a story.  The students write, Beatrice was a misunderstood ghost.  She wanted to make friends but kept scaring them by accident.  And the AI says, poor Beatrice, a lonely spirit yearning for companionship.  One day she stumbled upon an old abandoned mansion, et cetera, et cetera.  I encourage you all to, you know, hopefully one day try this.  This is surprisingly fun.  Now to even more directly hit this use case.  And what I'm about to show you, everything I showed you so far is actually already part of Calmigo.  What I'm about to show you, we haven't shown to anyone yet.\n",
        "\n",
        "# Enhancing Reading Comprehension and Writing Skills with AI\n",
        "\n",
        "This is a prototype. We hope to be able to launch it in the next few months.  But this is to directly use AI, use generative AI, to not undermine English and language arts but to actually enhance it in ways that we couldn't have even conceived of even a year ago.  This is reading comprehension.  This is the students reading Steve Jobs' famous speech at Stanford.  And then as they get to certain points, they can click on that little question.  And the AI will then, Socratically, almost like an oral exam, ask the student about things.  And the AI can highlight parts of the passage.  Why did the author use that word? What was their intent? Does it back up their argument? They can start to do stuff that, once again, we never had the capability to give everyone a tutor, everyone a writing coach, to actually dig into reading at this level.  And you could go on the other side of it.  We have a whole workflow that helps them write, helps them be a writing coach, draw an outline.  But once a student actually constructs a draft, and this is where they're constructing a draft, they can ask for feedback, once again, as you would expect from a good writing coach.  In this case, the student we'll say, let's say, does my evidence support my claim? And then the AI not only is able to give feedback, but it's able to highlight certain parts of the passage and says, you know, on this passage, this doesn't quite support your claim, but once again, Socratically says, can you tell us why? So it's pulling the student, it's making them a better writer, giving them far more feedback than they've ever been able to actually get before, and we think this is going to dramatically accelerate writing, not hurt it.\n",
        "\n",
        "§§§\n",
        "\n",
        "# Personalized Education for Teachers\n",
        "Now, everything I've talked about so far is for the student, but we think this could be equally as powerful for the teacher to drive more personalized education and, frankly, save time and energy for themselves and for their students. So this is an American history exercise on Khan Academy.  It's a question about the Spanish-American war.  And at first, it's in student mode, and if you say, tell me the answer, it's not going to tell the answer, it's going to go into tutoring mode.  But that little toggle which teachers have access to, they can turn student mode off, and then it goes into teacher mode.  And what this does is, it turns into, you could view it as a teacher's guide on steroids.  Not only can it explain the answer, it can explain how you might want to teach it.\n",
        "\n",
        "# Benefits for Teachers\n",
        "It can help prepare the teacher for that material. It can help them create lesson plans, as you can see doing right there.  It'll eventually help them create progress reports, it'll help them eventually grade.  So once again, teachers spend about half their time with this type of activity, lesson planning, all of that energy can go back to them or go back to human interactions with their actual students.  So, you know, one point I want to make, these large language models are so powerful, there's a temptation to say, well, all these people are just going to slap them onto their websites, and it kind of turns the applications themselves into commodities.  And what I've got to tell you is, I kind of thought that's one of the reasons why I didn't sleep for two weeks when I first had access to GPT-4 back in August.\n",
        "\n",
        "# Enhancing AI Tutoring\n",
        "But we quickly realized that to actually make it magical, I think it's really important to make it magical, but we quickly realized that to actually make it magical, I think what you saw with Conmigo a little bit, it didn't interact with you the way that you see chat GPT interacting, it was a little bit more magical, it was more Socratic, it was clearly much better at math than what most people are used to thinking. And the reason is there was a lot of work behind the scenes to make that happen.  And I could go through the whole list of everything we've been working on, many, many people, for over six, seven months, to make it feel magical, but perhaps the most intellectually interesting one is we realized, and this was an idea from an open AI researcher, that we could dramatically improve its ability in math and its ability in tutoring if we allowed the AI to think before it speaks.\n",
        "\n",
        "# The Future of AI and Education\n",
        "So if you're tutoring someone and you immediately just start talking before you assess their math, you might not get it right. But if you construct thoughts for yourself, and what you see on the right there is an actual AI thought, something that it generates for itself but it does not share with the student, then its accuracy went up dramatically and its ability to be a world-class tutor went up dramatically.  And you can see it's talking to itself here.  It says, the student got a different answer than I did, but do not tell them they made a mistake.  Instead, ask them to explain how they got to that step.  So I'll just finish off.\n",
        "\n",
        "# The Role of AI in Education and the Need for Positive Use Cases\n",
        "Hopefully, what I've just shown you is just half of what we are working on, and we think this is just the very tip of the iceberg of where this can actually go. And I'm pretty convinced, which I wouldn't have been even a year ago, that we, together, have a chance of addressing the two-sigma problem and turning it into a two-sigma opportunity, dramatically accelerating education as we know it.  Now, just to take a step back at a meta-level, obviously, we heard a lot today, the debates on either side.  There's folks who take a more pessimistic view of AI.  They say, this is scary, there's all these dystopian scenarios.  We maybe want to slow down.  We want to pause.  On the other side, there are the more optimistic folks who say, well, we've gone through inflection points before.\n",
        "\n",
        "We've gone through the Industrial Revolution. It was scary, but it all kind of worked out.  And what I'd argue right now is, I don't think this is like a flip of a coin or this is something where we'll just have to wait and see which way it turns out.  I think everyone here and beyond, we are active participants in this decision.  I'm pretty convinced that the first line of reasoning is actually almost a self-fulfilling prophecy, that if we act with fear and if we say, hey, we just got to stop doing this stuff, what's really going to happen is the rule followers might pause, might slow down, but the rule breakers, as Alexander mentioned, the totalitarian governments, the criminal organizations, they're only going to accelerate.\n",
        "\n",
        "And that leads to what I am pretty convinced is the dystopian state, which is the good actors have worse AIs than the bad actors. But I'll also talk to the optimist a little bit.  I don't think that means that, oh yeah, then we should just relax and just hope for the best.  That might not happen either.  I think all of us together have to fight like hell to make sure that we put the guardrails, we put in, when the problems arise, reasonable regulations, but we fight like hell for the positive use cases.  Because very close to my heart, and obviously there's many potential positive use cases, perhaps the most powerful use case, and perhaps the most poetic use case, is if AI, artificial intelligence, can be used to enhance HI, human intelligence, human potential and human purpose.  Thank you.  Thank you.  Thank you.\n",
        "'''"
      ],
      "metadata": {
        "id": "z9v5qGPi3JWQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = split_text_by_separator(my_text2, '\\n', '\\n', 'fix-nothing', 200)\n",
        "print(text_stats(my_text2))\n",
        "print(len(tmp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52U-Eixu4ESb",
        "outputId": "931cec0f-d5b7-4d24-e936-79a6dc3e84c1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words / paragraphs / §§§ segements: 3118 / 62 / 3\n",
            "\n",
            "Max words in §§§ segment: 1061 (1061 / 1027 / 1028)\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create summary\n",
        "\n",
        "from datetime import datetime\n",
        "import langchain\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (AIMessage, HumanMessage, SystemMessage)\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "\n",
        "# OpenAI Key\n",
        "openAI_key = creds['OpenAI']['v2']['credential']\n",
        "\n",
        "def create_summary_bullets (input_text, prompt_obj, lang, length='', length_max=1):\n",
        "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0, openai_api_key=openAI_key)\n",
        "  #llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0, openai_api_key=openAI_key)\n",
        "  # model overveiew see https://gptforwork.com/guides/openai-gpt3-models\n",
        "\n",
        "  #input_text_segments = input_text.split(\"§§§\")\n",
        "  input_text_segments = split_text_by_separator(input_text, '\\n', '\\n', 'fix-nothing', 1000)\n",
        "  output_text_segments = []\n",
        "  print('Started at: ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
        "  for index, input_text in enumerate(input_text_segments):\n",
        "\n",
        "      print('Processing step ' + str(index + 1) + ' of ' + str(len(input_text_segments)))\n",
        "\n",
        "      prompt_txt_final = build_prompt_from_template(input_text, prompt_obj, lang, length, length_max)\n",
        "      prompt_txt_final_FOR_LOG = build_prompt_from_template(input_text[:30], prompt_obj, lang, length, length_max)\n",
        "      print(textwrap.fill(prompt_txt_final_FOR_LOG, 120) + '\\n')\n",
        "\n",
        "      gpt_response_obj = llm.generate([[HumanMessage(content=prompt_txt_final)]])\n",
        "      tmp_text = gpt_response_obj.generations[0][0].text\n",
        "      tmp_tokens = gpt_response_obj.llm_output\n",
        "      #st.write(tmp_text)\n",
        "      print(str(tmp_tokens) + '\\n')\n",
        "      output_text_segments.append(tmp_text)\n",
        "\n",
        "  print('\\nEnded at: ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
        "  full_output_text = '\\n\\n'.join(output_text_segments)\n",
        "  return full_output_text\n",
        "\n",
        "prompt_obj = get_prompt_from_lib('sum', '2')\n",
        "print(json.dumps(prompt_obj, sort_keys=False, indent=2) + '\\n')\n",
        "result_text = create_summary_bullets(my_text2, prompt_obj, 'lang-de')\n",
        "# result_text = create_summary_bullets(my_text, prompt_obj, 'lang-de', 'length-max-dyn', 0.5)\n",
        "# result_text = create_summary_bullets(my_text, prompt_obj, 'lang-de', 'length-max-fix')\n",
        "print(result_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owpR0O6MTAEz",
        "outputId": "c9e818ae-592f-4e34-c6a6-fafc27e23ce2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"sum\",\n",
            "  \"description\": \"Max word length: 1100 for EN, 900 for DE. approx. 1 min processing time Example: Full book: 113 parts \\u00e1 1000 words takes 15 min.\",\n",
            "  \"category\": \"summarize\",\n",
            "  \"version\": \"2\",\n",
            "  \"note\": \"summarize prompt v2\",\n",
            "  \"prompt\": \"The given text is delimited by triple backticks. Summarize the current text to a maximum of 15 succint and clear bullet points of its contents. {language_text} {length_text} Text:```{input_text}```\",\n",
            "  \"lang-de\": \"Write everything in German language.\",\n",
            "  \"lang-en\": \"write everything in EN.\",\n",
            "  \"lang-same\": \"write in language of original text.\",\n",
            "  \"length-max-fix\": \"The maximum number of words should be 300 words in total.\",\n",
            "  \"length-max-dyn\": \"max length is {{length_calc(\\\"{input_text}\\\", {max_len})}} words.\"\n",
            "}\n",
            "\n",
            "Started at: 11:06:16\n",
            "\n",
            "Processing step 1 of 4\n",
            "The given text is delimited by triple backticks. Summarize the current text to a maximum of 15 succint and clear bullet\n",
            "points of its contents. Write everything in German language.  Text:```# The Potential of AI in Educa```\n",
            "\n",
            "Processing step 2 of 4\n",
            "The given text is delimited by triple backticks. Summarize the current text to a maximum of 15 succint and clear bullet\n",
            "points of its contents. Write everything in German language.  Text:```It understands the context of ```\n",
            "\n",
            "Processing step 3 of 4\n",
            "The given text is delimited by triple backticks. Summarize the current text to a maximum of 15 succint and clear bullet\n",
            "points of its contents. Write everything in German language.  Text:```This is a prototype. We hope t```\n",
            "\n",
            "Processing step 4 of 4\n",
            "The given text is delimited by triple backticks. Summarize the current text to a maximum of 15 succint and clear bullet\n",
            "points of its contents. Write everything in German language.  Text:```Hopefully, what I've just show```\n",
            "\n",
            "\n",
            "Ended at: 11:06:16\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test promptlib and templates\n",
        "\n",
        "import textwrap\n",
        "\n",
        "prompt_obj = get_prompt_from_lib('sum', '2')\n",
        "# prompt_obj = get_prompt_from_lib('sum', '1')\n",
        "# print(json.dumps(prompt_obj, sort_keys=True, indent=2))\n",
        "\n",
        "input_text = 'This is the input text. Lorem ipsum.'\n",
        "\n",
        "prompt_txt_final = build_prompt_from_template(input_text, prompt_obj, 'lang-de', 'length-max-dyn', 0.5)\n",
        "prompt_txt_final = build_prompt_from_template(input_text, prompt_obj, 'lang-de', 'length-max-fix')\n",
        "prompt_txt_final = build_prompt_from_template(input_text, prompt_obj, 'lang-de')\n",
        "print(textwrap.fill(prompt_txt_final, 120))\n",
        "\n",
        "#write_prompt_to_lib(prompt_obj)\n",
        "\n",
        "#print(json.dumps(promptlib, sort_keys=True, indent=2))\n",
        "\n",
        "# myprompt = \"The given text is delimited by triple backticks. Summarize the current text to succint and clear bullet points of its contents.\n",
        "#          The length of the summary must be 200 words maximum.\"\n",
        "# myprompt = \"The given text is delimited by triple backticks.\n",
        "#           Summarize the current text to a maximum of 15 succint and clear bullet points of its contents.\"\n",
        "# myprompt = myprompt + \"The maximum number of words should be 300 words in total. \"\n",
        "# myprompt = myprompt + \"Write everything in German language. \" # this is optional !\n",
        "# myprompt = myprompt + \"```\" + input_text + \"```\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsYX9xQ2mjZd",
        "outputId": "dc2abae6-85ec-4b85-c67d-91aae4256d56"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The given text is delimited by triple backticks. Summarize the current text to a maximum of 15 succint and clear bullet\n",
            "points of its contents. Write everything in German language.  Text:```This is the input text. Lorem ipsum.```\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0guuluULRqHaFzZtn8zUP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "902225593f3b40eea134dc76f3f9d808": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_d1ce91b752544c9aa636f98938dd879b",
            "msg_id": "",
            "outputs": []
          }
        },
        "d1ce91b752544c9aa636f98938dd879b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}